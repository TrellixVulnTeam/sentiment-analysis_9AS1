{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# System Configurations\n",
                "<b>RAM</b> : 32GB DDR4 2133MHz\n",
                "\n",
                "<b>GPU</b> : RTX 3060 12GB VRAM\n",
                "\n",
                "<b>CPU</b> : AMD Ryzen 5 3600XT 6-core\n",
                "\n",
                "<b>OS build</b> : Windows 10 Pro Build 21354.co_release.210402-1630"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "role: arn:aws:iam::183640780274:role/syalabi-user\n",
                        "region: ap-southeast-1\n",
                        "processing_repo: 183640780274.dkr.ecr.ap-southeast-1.amazonaws.com/sentiment-training:latest\n"
                    ]
                }
            ],
            "source": [
                "import boto3\n",
                "import sagemaker\n",
                "\n",
                "# Inference variables\n",
                "role = sagemaker.get_execution_role()\n",
                "bucket_name = 'syalabi-bucket'\n",
                "sess = sagemaker.session.Session(default_bucket=bucket_name)\n",
                "\n",
                "# AWS CLI arguments\n",
                "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
                "region = boto3.Session().region_name\n",
                "ecr_repo = 'sentiment-training'\n",
                "tag = ':latest'\n",
                "processing_repo_uri = f'{account_id}.dkr.ecr.{region}.amazonaws.com/{ecr_repo + tag}'\n",
                "\n",
                "print('role:', role)\n",
                "print('region:', region)\n",
                "print('processing_repo:', processing_repo_uri)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# requirements.txt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Overwriting docker/requirements.txt\n"
                    ]
                }
            ],
            "source": [
                "%%writefile docker/requirements.txt\n",
                "pandas\n",
                "transformers\n",
                "tensorflow\n",
                "scikit-learn\n",
                "tensorflow-addons"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Dockerfile"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Overwriting docker/Dockerfile\n"
                    ]
                }
            ],
            "source": [
                "%%writefile docker/Dockerfile\n",
                "# Dockerfile for local v1\n",
                "FROM nvcr.io/nvidia/tensorrt:21.03-py3\n",
                "COPY requirements.txt requirements.txt\n",
                "RUN pip3 install -r requirements.txt\n",
                "LABEL maintainer=\"Xaltius\"\n",
                "ENV PYTHONUNBUFFERED=TRUE\n",
                "ENV TOKENIZERS_PARALLELISM=false\n",
                "RUN mkdir -p /opt/ml/processing/model\n",
                "COPY . /usr/src/app\n",
                "COPY input/train_cleaned.csv /opt/ml/processing/input/train_cleaned.csv\n",
                "RUN mkdir -p /opt/ml/processing/model\n",
                "RUN mkdir -p /opt/ml/processing/output\n",
                "ENTRYPOINT [\"python3\", \"/usr/src/app/training.py\", \"--access_id=AKIASVQOX5XZJMYPBX3T\", \"--access_key=JRhjq9NwbJF1AwKxPgZvRe2ffvkuHuTH+nCKnimN\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Overwriting docker/Dockerfile\n"
                    ]
                }
            ],
            "source": [
                "%%writefile docker/Dockerfile\n",
                "# Dockerfile for cloud v1\n",
                "FROM nvcr.io/nvidia/tensorrt:21.03-py3\n",
                "RUN apt-get update && apt install -y \\\n",
                "    nvidia-utils-450 \\\n",
                "    ubuntu-drivers-common\n",
                "COPY requirements.txt requirements.txt\n",
                "RUN pip3 install -r requirements.txt\n",
                "LABEL maintainer=\"Xaltius\"\n",
                "ENV PYTHONUNBUFFERED=TRUE\n",
                "ENV TOKENIZERS_PARALLELISM=false\n",
                "RUN mkdir -p /opt/ml/processing/model\n",
                "RUN mkdir -p /opt/ml/processing/output\n",
                "COPY . /usr/src/app"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create ECR repo and push docker image\n",
                "!docker build --no-cache -t $ecr_repo docker\n",
                "!aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin {account_id}.dkr.ecr.{region}.amazonaws.com\n",
                "# !aws ecr create-repository --repository-name $ecr_repo\n",
                "!docker tag {ecr_repo + tag} $processing_repo_uri\n",
                "!docker push $processing_repo_uri\n",
                "!docker image rm $ecr_repo"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# training.py"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import argparse\n",
                "import subprocess\n",
                "import tensorflow as tf\n",
                "\n",
                "from helper_functions import *\n",
                "\n",
                "# Environ set-up\n",
                "tf.get_logger().setLevel('ERROR')\n",
                "\n",
                "SEED = 42\n",
                "seed_everything(SEED)\n",
                "\n",
                "print(\"INFO -- Dependencies\")\n",
                "print(subprocess.check_output('nvcc --version'.split(' ')).decode())\n",
                "print(\"Python version:\", sys.version)\n",
                "print(\"Tensorflow version:\", tf.__version__)\n",
                "\n",
                "\n",
                "if __name__ == '__main__':\n",
                "    parser = argparse.ArgumentParser()\n",
                "    parser.add_argument(\n",
                "        \"--access_id\", dest=\"access_id\", type=str)\n",
                "    parser.add_argument(\n",
                "        \"--access_key\", dest=\"access_key\", type=str)\n",
                "    parser.add_argument(\n",
                "        \"--max_length\", dest=\"max_len\", type=int, default=128)\n",
                "    parser.add_argument(\n",
                "        \"--num_labels\", dest=\"num_labels\", type=int, default=3)\n",
                "    parser.add_argument(\n",
                "        \"--batch_size\", dest=\"batch_size\", type=int, default=32)\n",
                "    parser.add_argument(\n",
                "        \"--epochs\", dest=\"epochs\", type=int, default=1)\n",
                "    parser.add_argument(\n",
                "        \"--lr\", dest='lr', type=float, default=1e-5)\n",
                "    parser.add_argument(\n",
                "        \"--splits\", dest='splits', type=int, default=2)\n",
                "    parser.add_argument(\n",
                "        \"--verbose\", dest='verbose', type=int, default=1)\n",
                "    parser.add_argument(\n",
                "        \"--pretrained_QA\", dest='pretrained_QA', type=str, \n",
                "        default='bert-base-uncased')\n",
                "    parser.add_argument(\n",
                "        \"--pretrained_PO\", dest='pretrained_PO', type=str, \n",
                "        default='distilroberta-base')   \n",
                "    parser.add_argument(\n",
                "        \"--bucket_name\", dest=\"bucket_name\", type=str, \n",
                "        default=\"syalabi-bucket\")\n",
                "\n",
                "    args, _ = parser.parse_known_args()\n",
                "\n",
                "    try: \n",
                "        train_data = get_data('/opt/ml/processing/input/train_cleaned.csv')\n",
                "        print(\"INFO -- Successfully fetched data from S3 bucket.\")\n",
                "    except:\n",
                "        print('ERROR -- Fetching data from S3 bucket has failed.')\n",
                "\n",
                "    QA_best_score, PO_best_score = 0, 0\n",
                "    skf = StratifiedKFold(n_splits=args.splits, shuffle=True, random_state=SEED)\n",
                "    for fold, (train_idx, val_idx) in enumerate(skf.split(\n",
                "            X=train_data.drop('sentiment', axis=1),\n",
                "            y=train_data['sentiment'])):\n",
                "        \n",
                "        QAtrain = get_input_arrays(train_data.loc[train_idx, :].reset_index(drop=True), type=\"QA\")\n",
                "        QAval = get_input_arrays(train_data.loc[val_idx, :].reset_index(drop=True), type=\"QA\")\n",
                "\n",
                "        POtrain = get_input_arrays(train_data.loc[train_idx, :].reset_index(drop=True), type=\"PO\")\n",
                "        POval = get_input_arrays(train_data.loc[train_idx, :].reset_index(drop=True), type=\"PO\")\n",
                "\n",
                "        # Phrase Loop\n",
                "        print(f'INFO -- Phrase Training Fold {fold+1} of {args.splits}.')\n",
                "        QAmodel = QAtraining(\n",
                "            model=QAModel(\n",
                "                pretrained=args.pretrained_QA, \n",
                "                lr=args.lr, \n",
                "                max_len=args.max_len),\n",
                "            epochs=args.epochs,\n",
                "            batch_size=args.batch_size,\n",
                "            train_inputs=QAtrain,\n",
                "            val_inputs=QAval,\n",
                "            verbose=args.verbose)\n",
                "\n",
                "        QA_train_score = get_Jaccard_score(\n",
                "            inputs=QAtrain,\n",
                "            model=QAmodel,\n",
                "            verbose=args.verbose)\n",
                "        \n",
                "        QA_val_score = get_Jaccard_score(\n",
                "            inputs=QAval,\n",
                "            model=QAmodel,\n",
                "            verbose=args.verbose)\n",
                "\n",
                "        print(f\"jaccard: {QA_train_score}% - val_jaccard: {QA_val_score}%\")   \n",
                "        \n",
                "        if QA_val_score > QA_best_score:\n",
                "            QA_best_score = QA_val_score\n",
                "            QAmodel.save_weights('/opt/ml/processing/model/phrase_model.h5')\n",
                "            print(f\"INFO -- FOLD {fold+1} phrase model weights saved.\")\n",
                "\n",
                "        tf.keras.backend.clear_session()\n",
                "        # Polarity Loop\n",
                "        print(f'INFO -- Polarity Training Fold {fold+1} of {args.splits}.')\n",
                "        POmodel = POtraining(\n",
                "            model=POModel(\n",
                "                pretrained=args.pretrained_PO,\n",
                "                lr=args.lr,\n",
                "                max_len=args.max_len,\n",
                "                num_labels=args.num_labels),\n",
                "            epochs=args.epochs,\n",
                "            batch_size=args.batch_size,\n",
                "            train_inputs=POtrain,\n",
                "            val_inputs=POval,\n",
                "            verbose=args.verbose)\n",
                "\n",
                "        PO_train_score = get_f1_score(\n",
                "            inputs=POtrain, \n",
                "            model=POmodel,\n",
                "            verbose=args.verbose)\n",
                "\n",
                "        PO_val_score = get_f1_score(\n",
                "            inputs=POval, \n",
                "            model=POmodel,\n",
                "            verbose=args.verbose)\n",
                "\n",
                "        print(f\"f1: {PO_train_score}% - val_f1: {PO_val_score}%\")\n",
                "\n",
                "        if PO_val_score > PO_best_score:\n",
                "            PO_best_score = PO_val_score\n",
                "            POmodel.save_weights('/opt/ml/processing/model/polarity_model.h5')\n",
                "            print(f\"INFO -- FOLD {fold+1} polarity model weights saved.\")\n",
                "\n",
                "    try: \n",
                "        save_tarfile(\n",
                "            outfile=\"/opt/ml/processing/output/sentiment_models.tar.gz\",\n",
                "            model_dir=\"/opt/ml/processing/model\")\n",
                "        print(\"INFO -- Successfully compressed models into tar.\")\n",
                "    except:\n",
                "        print(\"ERROR -- Failed to compress models.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# helper_functions.py"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %%writefile docker/helper_functions.py\n",
                "import os\n",
                "import re\n",
                "import ast\n",
                "import numpy as np\n",
                "import tensorflow as tf\n",
                "import pandas as pd\n",
                "\n",
                "from transformers import TFAutoModelForQuestionAnswering, AutoConfig\n",
                "\n",
                "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
                "\n",
                "from tensorflow.keras.layers import Input, Flatten, Conv1D, Activation\n",
                "from tensorflow.keras.losses import CategoricalCrossentropy\n",
                "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
                "\n",
                "\n",
                "def get_data(input_path):\n",
                "    train_data = pd.read_csv(input_path)\n",
                "    for col in train_data.columns[3:]:\n",
                "        train_data[col] = train_data[col].apply(ast.literal_eval)\n",
                "    return train_data\n",
                "\n",
                "\n",
                "def get_input_arrays(df):\n",
                "    ids = np.vstack(df['input_ids'])\n",
                "    mask = np.vstack(df['attention_mask'])\n",
                "    start = np.vstack(df['start_token'])\n",
                "    end = np.vstack(df['end_token'])\n",
                "    sentiment = np.vstack(df['sentiment_token'])\n",
                "    return ids, mask, start, end, sentiment\n",
                "\n",
                "\n",
                "def QAModel(max_len, pretrained, lr):\n",
                "    input_ids = Input(shape=(max_len,), dtype=tf.int32)\n",
                "    att_mask = Input(shape=(max_len,), dtype=tf.int32)\n",
                "    sent_mask = Input(shape=(max_len,), dtype=tf.int32)\n",
                "    \n",
                "    config = AutoConfig.from_pretrained(\n",
                "        pretrained, \n",
                "        output_attention=True, \n",
                "        output_hidden_states=True, \n",
                "        use_cache=True)\n",
                "\n",
                "    enc = TFAutoModelForQuestionAnswering.from_pretrained(\n",
                "        pretrained, config=config)\n",
                "    x = enc(input_ids, attention_mask=att_mask, token_type_ids=sent_mask)\n",
                "\n",
                "    x1 = tf.expand_dims(x[0], axis=-1)\n",
                "    x1 = Conv1D(1,1)(x1)\n",
                "    x1 = Flatten()(x[0])\n",
                "    x1 = Activation('softmax')(x1)\n",
                "\n",
                "    x2 = tf.expand_dims(x[1], axis=-1)\n",
                "    x2 = Conv1D(1,1)(x2)\n",
                "    x2 = Flatten()(x2)\n",
                "    x2 = Activation('softmax')(x2)\n",
                "\n",
                "    model = tf.keras.Model(inputs=[input_ids, att_mask, sent_mask], outputs=[x1,x2])\n",
                "\n",
                "    model.compile(\n",
                "        loss=CategoricalCrossentropy(label_smoothing=0.1),\n",
                "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr))\n",
                "\n",
                "    for layer in model.layers[3:4]:\n",
                "        layer.trainable = False\n",
                "\n",
                "    return model\n",
                "\n",
                "\n",
                "def model_training(model, epochs, save_path, batch_size, train_inputs, val_inputs):\n",
                "    \n",
                "    save_weights = ModelCheckpoint(\n",
                "        filepath=save_path,\n",
                "        monitor='val_loss',\n",
                "        verbose=1,\n",
                "        save_best_only=True,\n",
                "        save_weights_only=True,\n",
                "        mode='min')\n",
                "\n",
                "    early_stopping = EarlyStopping(\n",
                "        monitor='val_loss',\n",
                "        min_delta=0,\n",
                "        patience=2,\n",
                "        verbose=1,\n",
                "        mode='min',\n",
                "        restore_best_weights=True)\n",
                "\n",
                "    model.fit(\n",
                "        x=[train_inputs[0], train_inputs[1], train_inputs[4]],\n",
                "        y=[train_inputs[2], train_inputs[3]],\n",
                "        validation_data=(\n",
                "            [val_inputs[0], val_inputs[1], val_inputs[4]],\n",
                "            [val_inputs[2], val_inputs[3]]),\n",
                "        epochs=epochs,\n",
                "        batch_size=batch_size,\n",
                "        verbose=2,\n",
                "        callbacks=[save_weights, early_stopping])\n",
                "    \n",
                "    return model\n",
                "\n",
                "\n",
                "def get_score(\n",
                "    model, inputs):\n",
                "    preds = model.predict(\n",
                "        [inputs[0], inputs[1], inputs[4]], \n",
                "        verbose=1)\n",
                "\n",
                "    def JaccardSim(y_true, y_pred):\n",
                "        if (len(y_true)==0) & (len(y_pred)==0):\n",
                "            score = 0.5\n",
                "        else:\n",
                "            intersect = y_true.intersection(y_pred)\n",
                "            score = float(len(intersect)) / (len(y_true) + len(y_pred) - len(intersect))\n",
                "        return score\n",
                "\n",
                "    scores = []\n",
                "    for row in range(len(inputs[0])):\n",
                "        input = inputs[0][row]\n",
                "        t_start_idx = np.argmax(inputs[2][row], axis=-1)\n",
                "        t_end_idx = np.argmax(inputs[3][row], axis=-1)\n",
                "        t_output = set(input[t_start_idx:t_end_idx])\n",
                "\n",
                "        p_start_idx = np.argmax(preds[0][row], axis=-1)\n",
                "        p_end_idx = np.argmax(preds[1][row], axis=-1)\n",
                "        p_output = set(input[p_start_idx:p_end_idx])\n",
                "\n",
                "        score = JaccardSim(y_true=t_output, y_pred=p_output)\n",
                "        scores.append(score)\n",
                "\n",
                "    return round(np.mean(scores)*100, 2)"
            ]
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "b851d2923cfa3a2562599062e05fd9893d86a7c009c64d8ad3756552e4dd5f41"
        },
        "kernelspec": {
            "display_name": "Python 3.8.10 64-bit ('tf-gpu': conda)",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
