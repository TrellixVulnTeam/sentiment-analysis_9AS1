{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\r\n",
    "\r\n",
    "def PolarityTokenizer(text, tokenizer, max_length):\r\n",
    "    \"\"\"\r\n",
    "        Tokenizes text using the polarity tokenizer and\r\n",
    "        returns the input arrays for polarity model.\r\n",
    "\r\n",
    "        :param text: list of str\r\n",
    "        :param tokenizer: transformers.PreTrainedTokenizer object\r\n",
    "        :return: list of arrays\r\n",
    "    \"\"\"\r\n",
    "    inputs = tokenizer.encode_plus(\r\n",
    "        text=text,\r\n",
    "        add_special_tokens=True,\r\n",
    "        padding='max_length',\r\n",
    "        truncation=True,\r\n",
    "        max_length=max_length,\r\n",
    "        is_split_into_words=False,\r\n",
    "        return_tensors='tf',\r\n",
    "        return_attention_mask=True)\r\n",
    "\r\n",
    "    input_id = np.array(inputs['input_ids'].numpy()[0]).reshape((1, -1))\r\n",
    "    att_mask = np.array(inputs['attention_mask'].numpy()[0]).reshape((1, -1))\r\n",
    "\r\n",
    "    return [input_id, att_mask]\r\n",
    "\r\n",
    "\r\n",
    "def PhraseTokenizer(text, sentiment, tokenizer, max_length):\r\n",
    "    \"\"\"\r\n",
    "        Tokenizes text and sentiments and returns the\r\n",
    "        input arrays for phrase model.\r\n",
    "\r\n",
    "        :param text: list of str\r\n",
    "        :param sentiment: list of str\r\n",
    "        :param tokenizer: transformers.PreTrainedTokenizer object\r\n",
    "        :return: list of arrays\r\n",
    "    \"\"\"\r\n",
    "    inputs = tokenizer.encode_plus(\r\n",
    "        text=text,\r\n",
    "        text_pair=sentiment,\r\n",
    "        add_special_tokens=True,\r\n",
    "        padding='max_length',\r\n",
    "        truncation=True,\r\n",
    "        max_length=max_length,\r\n",
    "        is_split_into_words=False,\r\n",
    "        return_tensors='tf',\r\n",
    "        return_attention_mask=True)\r\n",
    "\r\n",
    "    input_id = list(inputs['input_ids'].numpy()[0])\r\n",
    "    att_mask = list(inputs['attention_mask'].numpy()[0])\r\n",
    "\r\n",
    "    sent_enc = tokenizer.encode(\r\n",
    "        text=sentiment,\r\n",
    "        add_special_tokens=False)\r\n",
    "\r\n",
    "    sent_idx = input_id.index(sent_enc[0])\r\n",
    "    sent_mask = np.zeros(max_length, dtype='int32')\r\n",
    "    sent_mask[sent_idx] = 1\r\n",
    "\r\n",
    "    input_id = np.array(input_id).reshape((1, -1))\r\n",
    "    att_mask = np.array(att_mask).reshape((1, -1))\r\n",
    "    sent_mask = np.array(sent_mask).reshape((1, -1))\r\n",
    "\r\n",
    "    return [input_id, att_mask, sent_mask]\r\n",
    "\r\n",
    "\r\n",
    "def PhraseDecoder(input_ids, prediction, tokenizer):\r\n",
    "    \"\"\"\r\n",
    "        Decodes predicted start and end tokens from phrase\r\n",
    "        model and returns the raw text string form\r\n",
    "\r\n",
    "        :param input_ids: array\r\n",
    "        :param prediction: list of arrays\r\n",
    "        :param tokenizer: transformers.PreTrainedTokenizer object\r\n",
    "        :return: str\r\n",
    "    \"\"\"\r\n",
    "    start_idx = np.argmax(prediction[0], axis=-1)[0]\r\n",
    "    end_idx = np.argmax(prediction[1], axis=-1)[0]\r\n",
    "    selected_text = input_ids[start_idx:end_idx]\r\n",
    "    selected_text = tokenizer.decode(selected_text).strip()\r\n",
    "    return selected_text\r\n",
    "\r\n",
    "\r\n",
    "def get_outputs(texts, polarity_tokenizer, phrase_tokenizer, polarity_model, phrase_model, POmax_len, QAmax_len):\r\n",
    "    sentiments = []\r\n",
    "    selected_texts = []\r\n",
    "    for text in texts:\r\n",
    "        polarity_inputs = PolarityTokenizer(\r\n",
    "            text=text,\r\n",
    "            tokenizer=polarity_tokenizer,\r\n",
    "            max_length=POmax_len)\r\n",
    "        polarity_preds = polarity_model.predict(polarity_inputs)\r\n",
    "        polarity_preds = np.argmax(polarity_preds, axis=1)\r\n",
    "        if polarity_preds == 0:\r\n",
    "            sentiment = 'negative'\r\n",
    "        elif polarity_preds == 1:\r\n",
    "            sentiment = 'neutral'\r\n",
    "        else:\r\n",
    "            sentiment = 'positive'            \r\n",
    "        phrase_inputs = PhraseTokenizer(\r\n",
    "            text=text,\r\n",
    "            sentiment=sentiment,\r\n",
    "            tokenizer=phrase_tokenizer,\r\n",
    "            max_length=QAmax_len)\r\n",
    "        phrase_ids = phrase_inputs[0][0]\r\n",
    "        phrase_preds = phrase_model.predict(phrase_inputs)\r\n",
    "        selected_text = PhraseDecoder(\r\n",
    "            input_ids=phrase_ids, \r\n",
    "            prediction=phrase_preds, \r\n",
    "            tokenizer=phrase_tokenizer)\r\n",
    "        sentiments.append(sentiment)\r\n",
    "        selected_texts.append(selected_text)\r\n",
    "    return sentiments, selected_texts"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import tensorflow as tf\r\n",
    "from tensorflow.keras.models import Model\r\n",
    "from tensorflow.keras.optimizers import Adam\r\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\r\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\r\n",
    "from tensorflow.keras.layers import (\r\n",
    "    BatchNormalization, Dense, Dropout,\r\n",
    "    Input, Conv1D, Flatten, Activation)\r\n",
    "from transformers import (AutoConfig, TFAutoModelForSequenceClassification,\r\n",
    "    TFAutoModelForQuestionAnswering)\r\n",
    "\r\n",
    "\r\n",
    "def PolarityModel(model_path, max_len, num_classes):\r\n",
    "    \"\"\"\r\n",
    "    Returns the polarity model loaded with pretrained weights.\r\n",
    "\r\n",
    "    :return: tf.keras.Model object\r\n",
    "    \"\"\"\r\n",
    "    input_ids = tf.keras.layers.Input(shape=(max_len,), name='input_1', dtype=tf.int32)\r\n",
    "    att_mask = tf.keras.layers.Input(shape=(max_len,), name='input_2', dtype=tf.int32)\r\n",
    "\r\n",
    "    enc = TFAutoModelForSequenceClassification.from_pretrained('distilroberta-base', num_labels=3)\r\n",
    "    x = enc(input_ids, attention_mask=att_mask)[0]\r\n",
    "\r\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\r\n",
    "    x = tf.keras.layers.Dense(256, activation=None)(x)\r\n",
    "    x = tf.keras.layers.Dropout(0.4)(x)\r\n",
    "    x = tf.keras.layers.Dense(3, activation='softmax')(x)\r\n",
    "\r\n",
    "    model = tf.keras.Model(inputs=[input_ids, att_mask], outputs=x)\r\n",
    "\r\n",
    "    for layer in model.layers[:3]:\r\n",
    "        layer.trainable = True\r\n",
    "\r\n",
    "    model.load_weights(model_path)\r\n",
    "\r\n",
    "    return model\r\n",
    "\r\n",
    "\r\n",
    "def PhraseModel(model_path, max_len):\r\n",
    "    \"\"\"\r\n",
    "    Returns the phrase model loaded with pretrained weights.\r\n",
    "\r\n",
    "    :return: tf.keras.Model object\r\n",
    "    \"\"\"\r\n",
    "    input_ids = tf.keras.layers.Input(shape=(max_len,), name=\"input_1\",  dtype=tf.int32)\r\n",
    "    att_mask = tf.keras.layers.Input(shape=(max_len,), name=\"input_2\", dtype=tf.int32)\r\n",
    "    sent_mask = tf.keras.layers.Input(shape=(max_len,), name=\"input_3\", dtype=tf.int32)\r\n",
    "    \r\n",
    "    config = AutoConfig.from_pretrained(\r\n",
    "        'bert-base-uncased', \r\n",
    "        output_attention=True, \r\n",
    "        output_hidden_states=True, \r\n",
    "        use_cache=True)\r\n",
    "\r\n",
    "    enc = TFAutoModelForQuestionAnswering.from_pretrained(\r\n",
    "        'bert-base-uncased', config=config)\r\n",
    "    x = enc(input_ids, attention_mask=att_mask, token_type_ids=sent_mask)\r\n",
    "\r\n",
    "    x1 = tf.keras.layers.Dropout(0.1)(x[0])\r\n",
    "    x1 = tf.expand_dims(x1, axis=-1)\r\n",
    "    x1 = tf.keras.layers.Conv1D(1,1)(x1)\r\n",
    "    x1 = tf.keras.layers.Flatten()(x[0])\r\n",
    "    x1 = tf.keras.layers.Activation('softmax')(x1)\r\n",
    "\r\n",
    "    x2 = tf.keras.layers.Dropout(0.1)(x[1])\r\n",
    "    x2 = tf.expand_dims(x2, axis=-1)\r\n",
    "    x2 = tf.keras.layers.Conv1D(1,1)(x2)\r\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\r\n",
    "    x2 = tf.keras.layers.Activation('softmax')(x2)\r\n",
    "\r\n",
    "    model = tf.keras.Model(inputs=[input_ids, att_mask, sent_mask], outputs=[x1,x2])\r\n",
    "\r\n",
    "    for layer in model.layers[3:4]:\r\n",
    "        layer.trainable = True\r\n",
    "\r\n",
    "    model.load_weights(model_path)\r\n",
    "\r\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import os\r\n",
    "import ast\r\n",
    "import argparse\r\n",
    "import pandas as pd\r\n",
    "import tarfile\r\n",
    "import sys\r\n",
    "import subprocess\r\n",
    "import datetime\r\n",
    "\r\n",
    "import tensorflow as tf\r\n",
    "from transformers import AutoTokenizer\r\n",
    "import transformers\r\n",
    "\r\n",
    "# Mute warnings\r\n",
    "tf.get_logger().setLevel('ERROR')\r\n",
    "\r\n",
    "print(subprocess.check_output('nvcc --version'.split(' ')).decode())\r\n",
    "print(sys.version)\r\n",
    "print(tf.__version__)\r\n",
    "print(transformers.__version__)\r\n",
    "print(pd.__version__)\r\n",
    "\r\n",
    "if __name__ == '__main__':\r\n",
    "    parser = argparse.ArgumentParser(description=\"For S3 bucket access\")\r\n",
    "    parser.add_argument(\r\n",
    "        \"--access_id\", dest='access_id', type=str)\r\n",
    "    parser.add_argument(\r\n",
    "        \"--access_key\", dest='access_key', type=str)\r\n",
    "    parser.add_argument(\r\n",
    "        \"--num_classes\", dest='num_classes', type=int, default=3)\r\n",
    "    parser.add_argument(\r\n",
    "        \"--QAmax_len\", dest='QAmax_len', type=int, default=64)\r\n",
    "    parser.add_argument(\r\n",
    "        \"--POmax_len\", dest='POmax_len', type=int, default=128)    \r\n",
    "    parser.add_argument(\r\n",
    "        \"--bucket_name\", dest='bucket_name', type=str, default='syalabi-bucket')\r\n",
    "\r\n",
    "    args, _ = parser.parse_known_args()\r\n",
    "\r\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\r\n",
    "        'roberta-base',\r\n",
    "        add_prefix_space=True)\r\n",
    "    \r\n",
    "    print(\"INFO -- Tokenizers initialized.\")\r\n",
    "\r\n",
    "    # Input\r\n",
    "    input_data_path = \"/opt/ml/processing/input/input.csv\"\r\n",
    "    data = pd.read_csv(input_data_path)\r\n",
    "\r\n",
    "    print(\"INFO -- Input data initialized.\")\r\n",
    "\r\n",
    "    model_path = os.path.join(\r\n",
    "        \"/opt/ml/processing/model/sentiment_models.tar.gz\")\r\n",
    "\r\n",
    "    with tarfile.open(model_path) as tar:\r\n",
    "        tar.extractall(path=\"/opt/ml/processing/model/\")\r\n",
    "\r\n",
    "    polarity_model = PolarityModel(\r\n",
    "        model_path='/opt/ml/processing/model/polarity_model.h5',\r\n",
    "        max_len=args.POmax_len,\r\n",
    "        num_classes=args.num_classes)\r\n",
    "\r\n",
    "    print(\"INFO -- Polarity model initialized.\")\r\n",
    "   \r\n",
    "    phrase_model = PhraseModel(\r\n",
    "        model_path='/opt/ml/processing/model/phrase_model.h5',\r\n",
    "        max_len=args.QAmax_len)\r\n",
    "\r\n",
    "    print(\"INFO -- Phrase model initialized.\")    \r\n",
    "\r\n",
    "    # Main loop\r\n",
    "    all_sentiments = []\r\n",
    "    all_selected_texts = []\r\n",
    "    for texts in data['text']:\r\n",
    "        texts = ast.literal_eval(texts)\r\n",
    "        sentiments, selected_texts = get_outputs(\r\n",
    "            texts=texts, \r\n",
    "            polarity_tokenizer=tokenizer, \r\n",
    "            phrase_tokenizer=tokenizer,\r\n",
    "            polarity_model=polarity_model, \r\n",
    "            phrase_model=phrase_model,\r\n",
    "            POmax_len=args.POmax_len,\r\n",
    "            QAmax_len=args.QAmax_len)\r\n",
    "        all_sentiments.append(sentiments)\r\n",
    "        all_selected_texts.append(selected_texts)\r\n",
    "    \r\n",
    "    data['sentiments'] = all_sentiments\r\n",
    "    data['selected_texts'] = all_selected_texts\r\n",
    "    \r\n",
    "    # Output\r\n",
    "    time_stamp = datetime.datetime.now().strftime(\"%m%d%Y-%H%Mhrs\")\r\n",
    "    output_path = os.path.join(\r\n",
    "        \"/opt/ml/processing/output\", f\"output_{time_stamp}.csv\")\r\n",
    "    data.to_csv(\r\n",
    "        output_path,\r\n",
    "        index=False)\r\n",
    "\r\n",
    "    print(\"INFO -- Output saved in S3.\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2020 NVIDIA Corporation\n",
      "Built on Mon_Oct_12_20:54:10_Pacific_Daylight_Time_2020\n",
      "Cuda compilation tools, release 11.1, V11.1.105\n",
      "Build cuda_11.1.relgpu_drvr455TC455_06.29190527_0\n",
      "\n",
      "3.8.10 (default, May 19 2021, 13:12:57) [MSC v.1916 64 bit (AMD64)]\n",
      "2.5.0\n",
      "4.8.2\n",
      "1.3.0\n",
      "INFO -- Tokenizers initialized.\n",
      "INFO -- Input data initialized.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO -- Polarity model initialized.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForQuestionAnswering.\n",
      "\n",
      "Some layers of TFBertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO -- Phrase model initialized.\n",
      "INFO -- Output saved in S3.\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('tf-gpu': conda)"
  },
  "interpreter": {
   "hash": "b851d2923cfa3a2562599062e05fd9893d86a7c009c64d8ad3756552e4dd5f41"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}