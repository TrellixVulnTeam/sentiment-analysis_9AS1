{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\r\n",
    "import re\r\n",
    "import ast\r\n",
    "import tarfile\r\n",
    "import random\r\n",
    "import numpy as np\r\n",
    "import tensorflow as tf\r\n",
    "import tensorflow_addons as tfa\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "from transformers import (\r\n",
    "    TFAutoModelForQuestionAnswering, \r\n",
    "    TFAutoModelForSequenceClassification,\r\n",
    "    AutoConfig)\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\r\n",
    "from sklearn.metrics import confusion_matrix, f1_score\r\n",
    "\r\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\r\n",
    "\r\n",
    "\r\n",
    "def seed_everything(SEED):\r\n",
    "   random.seed(SEED)\r\n",
    "   np.random.seed(SEED)\r\n",
    "   tf.random.set_seed(SEED)\r\n",
    "   print(\"INFO -- Random seed set.\")\r\n",
    "\r\n",
    "\r\n",
    "def get_data(input_path, max_len):\r\n",
    "    train_data = pd.read_csv(input_path)\r\n",
    "    for col in train_data.columns[3:]:\r\n",
    "        train_data[col] = train_data[col].apply(ast.literal_eval).apply(np.array)\r\n",
    "        if col in train_data.columns[3:-2]:\r\n",
    "            train_data[col] = train_data[col].apply(lambda x: x[:max_len])\r\n",
    "    return train_data\r\n",
    "\r\n",
    "\r\n",
    "def get_input_arrays(df, type, batch_size, buffer_size, training_set):\r\n",
    "    ids = np.vstack(df['input_ids'])\r\n",
    "    mask = np.vstack(df['attention_mask'])\r\n",
    "    start = np.vstack(df['start_token'])\r\n",
    "    end = np.vstack(df['end_token'])\r\n",
    "    sentiment = np.vstack(df['sentiment_token'])\r\n",
    "    ids_PO = np.vstack(df['input_ids2'])\r\n",
    "    mask_PO = np.vstack(df['attention_mask2'])\r\n",
    "    sentiment_PO = df['sentiment'].astype('int8')\r\n",
    "\r\n",
    "    if type==\"QA\":\r\n",
    "        gen = tf.data.Dataset.from_tensor_slices((\r\n",
    "            {\r\n",
    "                \"input_1\": ids,\r\n",
    "                \"input_2\": mask,\r\n",
    "                \"input_3\": sentiment\r\n",
    "            },\r\n",
    "            {\r\n",
    "                \"output_1\": tf.convert_to_tensor(start, dtype=tf.int32),\r\n",
    "                \"output_2\": tf.convert_to_tensor(end, dtype=tf.int32)\r\n",
    "            }\r\n",
    "        ))\r\n",
    "\r\n",
    "    else:\r\n",
    "        gen = tf.data.Dataset.from_tensor_slices((\r\n",
    "            {\r\n",
    "                \"input_1\": ids_PO,\r\n",
    "                \"input_2\": mask_PO\r\n",
    "            }, tf.one_hot(sentiment_PO, depth=3)\r\n",
    "        ))\r\n",
    "\r\n",
    "    gen = gen.shuffle(buffer_size=256)\r\n",
    "    gen = gen.batch(batch_size=batch_size)\r\n",
    "    gen = gen.prefetch(buffer_size=buffer_size)\r\n",
    "    return gen\r\n",
    "\r\n",
    "\r\n",
    "def QAModel(pretrained, max_len):\r\n",
    "    input_ids = tf.keras.layers.Input(shape=(max_len,), name=\"input_1\",  dtype=tf.int32)\r\n",
    "    att_mask = tf.keras.layers.Input(shape=(max_len,), name=\"input_2\", dtype=tf.int32)\r\n",
    "    sent_mask = tf.keras.layers.Input(shape=(max_len,), name=\"input_3\", dtype=tf.int32)\r\n",
    "    \r\n",
    "    config = AutoConfig.from_pretrained(\r\n",
    "        pretrained, \r\n",
    "        output_attention=True, \r\n",
    "        output_hidden_states=True, \r\n",
    "        use_cache=True)\r\n",
    "\r\n",
    "    enc = TFAutoModelForQuestionAnswering.from_pretrained(\r\n",
    "        pretrained, config=config)\r\n",
    "    x = enc(input_ids, attention_mask=att_mask, token_type_ids=sent_mask)\r\n",
    "\r\n",
    "    x1 = tf.keras.layers.Dropout(0.1)(x[0])\r\n",
    "    x1 = tf.expand_dims(x1, axis=-1)\r\n",
    "    x1 = tf.keras.layers.Conv1D(1,1)(x1)\r\n",
    "    x1 = tf.keras.layers.Flatten()(x[0])\r\n",
    "    x1 = tf.keras.layers.Activation('softmax')(x1)\r\n",
    "\r\n",
    "    x2 = tf.keras.layers.Dropout(0.1)(x[1])\r\n",
    "    x2 = tf.expand_dims(x2, axis=-1)\r\n",
    "    x2 = tf.keras.layers.Conv1D(1,1)(x2)\r\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\r\n",
    "    x2 = tf.keras.layers.Activation('softmax')(x2)\r\n",
    "\r\n",
    "    model = tf.keras.Model(inputs=[input_ids, att_mask, sent_mask], outputs=[x1,x2])\r\n",
    "\r\n",
    "    for layer in model.layers[3:4]:\r\n",
    "        layer.trainable = True\r\n",
    "\r\n",
    "    return model\r\n",
    "\r\n",
    "\r\n",
    "def POModel(max_len, pretrained):\r\n",
    "    input_ids = tf.keras.layers.Input(shape=(max_len,), name='input_1', dtype=tf.int32)\r\n",
    "    att_mask = tf.keras.layers.Input(shape=(max_len,), name='input_2', dtype=tf.int32)\r\n",
    "\r\n",
    "    enc = TFAutoModelForSequenceClassification.from_pretrained(pretrained, num_labels=3)\r\n",
    "    x = enc(input_ids, attention_mask=att_mask)[0]\r\n",
    "\r\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\r\n",
    "    x = tf.keras.layers.Dense(256, activation=None)(x)\r\n",
    "    x = tf.keras.layers.Dropout(0.4)(x)\r\n",
    "    x = tf.keras.layers.Dense(3, activation='softmax')(x)\r\n",
    "\r\n",
    "    model = tf.keras.Model(inputs=[input_ids, att_mask], outputs=x)\r\n",
    "\r\n",
    "    for layer in model.layers[:3]:\r\n",
    "        layer.trainable = True\r\n",
    "\r\n",
    "    return model\r\n",
    "\r\n",
    "\r\n",
    "def QAtraining(epochs, max_len, learning_rate, train_gen, val_gen):\r\n",
    "    optimizer = tf.keras.optimizers.Adam(\r\n",
    "        learning_rate=tf.keras.optimizers.schedules.CosineDecayRestarts(\r\n",
    "            initial_learning_rate=learning_rate, \r\n",
    "            first_decay_steps=3,\r\n",
    "            t_mul=2.0, m_mul=1.0, alpha=0.0))\r\n",
    "\r\n",
    "    cce_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=False, label_smoothing=0.2)\r\n",
    "    model = QAModel(pretrained='bert-base-uncased', max_len=max_len)\r\n",
    "    t_jaccard_metric = tf.keras.metrics.MeanIoU(num_classes=2)\r\n",
    "    v_jaccard_metric = tf.keras.metrics.MeanIoU(num_classes=2)\r\n",
    "    metrics = ['loss', 'jaccard', 'val_loss', 'val_jaccard']\r\n",
    "    best_score = 0\r\n",
    "\r\n",
    "    t_jaccard_metric.reset_states()\r\n",
    "    v_jaccard_metric.reset_states()\r\n",
    "\r\n",
    "    @tf.function\r\n",
    "    def getArrays(array):\r\n",
    "        start_array = tf.where(\r\n",
    "            tf.sequence_mask(\r\n",
    "                lengths=tf.math.argmax(array[0], axis=1), \r\n",
    "                maxlen=max_len), \r\n",
    "            x=1, y=0)\r\n",
    "        end_array = tf.where(\r\n",
    "            tf.sequence_mask(\r\n",
    "                lengths=tf.math.argmax(array[1], axis=1),\r\n",
    "                maxlen=max_len),\r\n",
    "            x=1, y=0)\r\n",
    "        return tf.abs(end_array - start_array)\r\n",
    "\r\n",
    "    @tf.function\r\n",
    "    def train_step(X, y, loss_fn):\r\n",
    "        y = [y['output_1'], y['output_2']]\r\n",
    "        with tf.GradientTape() as tape:        \r\n",
    "            y_hat = model(X, training=True)\r\n",
    "            loss_value = loss_fn(y, y_hat)\r\n",
    "            loss_value += sum(model.losses)\r\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\r\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\r\n",
    "        t_jaccard_metric.update_state(getArrays(y), getArrays(y_hat))\r\n",
    "        return loss_value\r\n",
    "\r\n",
    "    @tf.function\r\n",
    "    def val_step(X, y, loss_fn):\r\n",
    "        y = [y['output_1'], y['output_2']]\r\n",
    "        y_hat = model(X, training=False)\r\n",
    "        loss_value = loss_fn(y, y_hat)\r\n",
    "        v_jaccard_metric.update_state(getArrays(y), getArrays(y_hat))\r\n",
    "        return loss_value\r\n",
    "\r\n",
    "    i = 0\r\n",
    "    for epoch in range(1, epochs+1):\r\n",
    "        print(f\"Epoch {epoch}/{epochs}\")\r\n",
    "        progbar = tf.keras.utils.Progbar(\r\n",
    "            len(train_gen), interval=0.5, stateful_metrics=metrics)\r\n",
    "        print(f\"INFO -- Learning rate set to {optimizer.lr(epoch)}.\")\r\n",
    "        for step, (X_train, y_train) in enumerate(train_gen):\r\n",
    "            train_loss = train_step(X_train, y_train, cce_fn)\r\n",
    "            progbar.update(step, values=[('loss', train_loss), ('jaccard', t_jaccard_metric.result())])\r\n",
    "\r\n",
    "        for x_batch_val, y_batch_val in val_gen:\r\n",
    "            val_loss = val_step(x_batch_val, y_batch_val, cce_fn)\r\n",
    "\r\n",
    "        values = [\r\n",
    "            ('loss', train_loss), \r\n",
    "            ('jaccard', t_jaccard_metric.result()),\r\n",
    "            ('val_loss', val_loss),\r\n",
    "            ('val_jaccard', v_jaccard_metric.result())]\r\n",
    "        progbar.update(len(train_gen), values=values, finalize=True)\r\n",
    "\r\n",
    "        if best_score < v_jaccard_metric.result():\r\n",
    "            best_score = v_jaccard_metric.result()\r\n",
    "            best_model = model\r\n",
    "            print(\"INFO -- Model improved.\\n\")\r\n",
    "            i = 0\r\n",
    "        else:\r\n",
    "            i += 1\r\n",
    "            if (i < 2) & (v_jaccard_metric.result() < t_jaccard_metric.result()):\r\n",
    "                print(\"INFO -- Model did not improve.\\n\")\r\n",
    "            else:\r\n",
    "                print(\"INFO -- Early Stopping.\\n\")\r\n",
    "                break\r\n",
    "\r\n",
    "    return best_model, best_score\r\n",
    "\r\n",
    "\r\n",
    "def POtraining(epochs, max_len, learning_rate, train_gen, val_gen):\r\n",
    "    optimizer = tf.keras.optimizers.Adam(\r\n",
    "        learning_rate=learning_rate)\r\n",
    "\r\n",
    "    cce_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=False, label_smoothing=0.2)\r\n",
    "    model = POModel(pretrained='distilroberta-base', max_len=max_len)\r\n",
    "    t_f1_metric = tfa.metrics.F1Score(num_classes=3, average='micro')\r\n",
    "    v_f1_metric = tfa.metrics.F1Score(num_classes=3, average='micro')\r\n",
    "    metrics = ['loss', 'f1', 'val_loss', 'val_f1']\r\n",
    "    best_score = 0\r\n",
    "\r\n",
    "    t_f1_metric.reset_states()\r\n",
    "    v_f1_metric.reset_states()\r\n",
    "\r\n",
    "\r\n",
    "    @tf.function\r\n",
    "    def train_step(X, y, loss_fn):\r\n",
    "        with tf.GradientTape() as tape:\r\n",
    "            y_hat = model(X, training=True)\r\n",
    "            loss_value = loss_fn(y, y_hat)\r\n",
    "            loss_value += sum(model.losses)\r\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\r\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\r\n",
    "        t_f1_metric.update_state(y, y_hat)\r\n",
    "        return loss_value\r\n",
    "\r\n",
    "    @tf.function\r\n",
    "    def val_step(X, y, loss_fn):\r\n",
    "        y_hat = model(X, training=False)\r\n",
    "        loss_value = loss_fn(y, y_hat)\r\n",
    "        v_f1_metric.update_state(y, y_hat)\r\n",
    "        return loss_value\r\n",
    "\r\n",
    "    i = 0\r\n",
    "    for epoch in range(1, epochs+1):\r\n",
    "        print(f\"Epoch {epoch}/{epochs}\")\r\n",
    "        progbar = tf.keras.utils.Progbar(\r\n",
    "            len(train_gen), interval=0.5, stateful_metrics=metrics)\r\n",
    "        for step, (X_train, y_train) in enumerate(train_gen):\r\n",
    "            train_loss = train_step(X_train, y_train, cce_fn)\r\n",
    "            progbar.update(step, values=[('loss', train_loss), ('f1', t_f1_metric.result())])\r\n",
    "\r\n",
    "        for x_batch_val, y_batch_val in val_gen:\r\n",
    "            val_loss = val_step(x_batch_val, y_batch_val, cce_fn)\r\n",
    "\r\n",
    "        values = [\r\n",
    "            ('loss', train_loss),\r\n",
    "            ('f1', t_f1_metric.result()),\r\n",
    "            ('val_loss', val_loss),\r\n",
    "            ('val_f1', v_f1_metric.result())]\r\n",
    "        progbar.update(len(train_gen), values=values, finalize=True)\r\n",
    "\r\n",
    "        if ((t_f1_metric.result() - v_f1_metric.result()) < 0.02) & (v_f1_metric.result() > best_score):\r\n",
    "            best_score = v_f1_metric.result()\r\n",
    "            best_model = model\r\n",
    "            print(\"INFO -- Model improved.\\n\")\r\n",
    "            i = 0\r\n",
    "        else:\r\n",
    "            i += 1\r\n",
    "            if i < 2:\r\n",
    "                print(\"INFO -- Model did not improve.\\n\")\r\n",
    "            else:\r\n",
    "                print(\"INFO -- Early Stopping.\\n\")\r\n",
    "                break\r\n",
    "\r\n",
    "    return best_model, best_score\r\n",
    "    \r\n",
    "\r\n",
    "def save_tarfile(outfile, model_dir):\r\n",
    "    with tarfile.open(outfile, \"w:gz\") as tar:\r\n",
    "        for file in os.listdir(model_dir):\r\n",
    "            tar.add(\r\n",
    "                name=os.path.join(model_dir, file), \r\n",
    "                arcname=file,\r\n",
    "                recursive=False)\r\n",
    "        tar.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import sys\r\n",
    "import argparse\r\n",
    "import subprocess\r\n",
    "import tensorflow as tf\r\n",
    "\r\n",
    "# Environ set-up\r\n",
    "tf.get_logger().setLevel('ERROR')\r\n",
    "\r\n",
    "print(\"------ Dependencies ------\")\r\n",
    "print(subprocess.check_output('nvcc --version'.split(' ')).decode())\r\n",
    "print(\"Python version:\", sys.version)\r\n",
    "print(\"Tensorflow version:\", tf.__version__)\r\n",
    "\r\n",
    "# Mixed precision\r\n",
    "try:\r\n",
    "    policy = tf.keras.mixed_precision.Policy('float32')\r\n",
    "    tf.keras.mixed_precision.set_global_policy(policy)\r\n",
    "    print(\"INFO -- Mixed precision set.\")\r\n",
    "except:\r\n",
    "    print(\"ERROR -- Mixed precision not set.\")\r\n",
    "\r\n",
    "SEED = 42\r\n",
    "seed_everything(SEED)\r\n",
    "buffer_size = tf.data.AUTOTUNE"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "------ Dependencies ------\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2020 NVIDIA Corporation\n",
      "Built on Mon_Oct_12_20:54:10_Pacific_Daylight_Time_2020\n",
      "Cuda compilation tools, release 11.1, V11.1.105\n",
      "Build cuda_11.1.relgpu_drvr455TC455_06.29190527_0\n",
      "\n",
      "Python version: 3.8.10 (default, May 19 2021, 13:12:57) [MSC v.1916 64 bit (AMD64)]\n",
      "Tensorflow version: 2.5.0\n",
      "INFO -- Mixed precision set.\n",
      "INFO -- Random seed set.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "if __name__ == '__main__':\r\n",
    "    parser = argparse.ArgumentParser()\r\n",
    "    parser.add_argument(\r\n",
    "        \"--access_id\", dest=\"access_id\", type=str)\r\n",
    "    parser.add_argument(\r\n",
    "        \"--access_key\", dest=\"access_key\", type=str)\r\n",
    "    parser.add_argument(\r\n",
    "        \"--max_length\", dest=\"max_len\", type=int, default=64)    \r\n",
    "    parser.add_argument(\r\n",
    "        \"--batch_size\", dest=\"batch_size\", type=int, default=32)\r\n",
    "    parser.add_argument(\r\n",
    "        \"--epochs\", dest=\"epochs\", type=int, default=20)\r\n",
    "    parser.add_argument(\r\n",
    "        \"--lr\", dest='lr', type=float, default=1e-5)\r\n",
    "    parser.add_argument(\r\n",
    "        \"--splits\", dest='splits', type=int, default=5)\r\n",
    "    parser.add_argument(\r\n",
    "        \"--pretrained_QA\", dest='pretrained_QA', type=str, \r\n",
    "        default='bert-base-uncased')\r\n",
    "    parser.add_argument(\r\n",
    "        \"--pretrained_PO\", dest='pretrained_PO', type=str, \r\n",
    "        default='distilroberta-base')   \r\n",
    "    parser.add_argument(\r\n",
    "        \"--bucket_name\", dest=\"bucket_name\", type=str, \r\n",
    "        default=\"syalabi-bucket\")\r\n",
    "\r\n",
    "    args, _ = parser.parse_known_args()\r\n",
    "\r\n",
    "    try: \r\n",
    "        train_data = get_data(\r\n",
    "            input_path='input/train_cleaned.csv',\r\n",
    "            max_len=args.max_len)\r\n",
    "        print(\"INFO -- Successfully fetched data from S3 bucket.\")\r\n",
    "    except:\r\n",
    "        print('ERROR -- Fetching data from S3 bucket has failed.')\r\n",
    "\r\n",
    "    QA_best_score, PO_best_score = 0, 0\r\n",
    "    skf = StratifiedKFold(n_splits=args.splits, shuffle=True, random_state=SEED)\r\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(\r\n",
    "            X=train_data.drop('sentiment', axis=1),\r\n",
    "            y=train_data['sentiment'])):\r\n",
    "        \r\n",
    "        QAtrain = get_input_arrays(\r\n",
    "            train_data.loc[train_idx, :].reset_index(drop=True), \r\n",
    "            type=\"QA\",\r\n",
    "            batch_size=args.batch_size,\r\n",
    "            buffer_size=buffer_size,\r\n",
    "            training_set=True)\r\n",
    "\r\n",
    "        QAval = get_input_arrays(\r\n",
    "            train_data.loc[val_idx, :].reset_index(drop=True),\r\n",
    "            type=\"QA\",\r\n",
    "            batch_size=args.batch_size,\r\n",
    "            buffer_size=buffer_size,\r\n",
    "            training_set=False)\r\n",
    "\r\n",
    "        POtrain = get_input_arrays(\r\n",
    "            train_data.loc[train_idx, :].reset_index(drop=True), \r\n",
    "            type=\"PO\",\r\n",
    "            batch_size=args.batch_size,\r\n",
    "            buffer_size=buffer_size,\r\n",
    "            training_set=True)\r\n",
    "\r\n",
    "        POval = get_input_arrays(\r\n",
    "            train_data.loc[val_idx, :].reset_index(drop=True), \r\n",
    "            type=\"PO\",\r\n",
    "            batch_size=args.batch_size,\r\n",
    "            buffer_size=buffer_size,\r\n",
    "            training_set=False)\r\n",
    "\r\n",
    "        # Phrase Loop\r\n",
    "        print(f'INFO -- Phrase Training Fold {fold+1} of {args.splits}.')\r\n",
    "        QAmodel, QA_val_score = QAtraining(\r\n",
    "            epochs=args.epochs, \r\n",
    "            max_len=args.max_len,\r\n",
    "            learning_rate=args.lr,\r\n",
    "            train_gen=QAtrain,\r\n",
    "            val_gen=QAval)\r\n",
    "       \r\n",
    "        if QA_val_score > QA_best_score:\r\n",
    "            QA_best_score = QA_val_score\r\n",
    "            QAmodel.save_weights('opt/ml/processing/model/phrase_model.h5')\r\n",
    "            print(f\"INFO -- FOLD {fold+1} phrase model weights saved.\")\r\n",
    "        else:\r\n",
    "            print(\"INFO -- Model weights were not replaced.\")\r\n",
    "\r\n",
    "        # Polarity Loop\r\n",
    "        tf.keras.backend.clear_session()\r\n",
    "        print(f'INFO -- Polarity Training Fold {fold+1} of {args.splits}.')\r\n",
    "        \r\n",
    "        POmodel, PO_val_score = POtraining(\r\n",
    "            epochs=args.epochs, \r\n",
    "            max_len=args.max_len,\r\n",
    "            learning_rate=args.lr,\r\n",
    "            train_gen=POtrain,\r\n",
    "            val_gen=POval)\r\n",
    "\r\n",
    "        if PO_val_score > PO_best_score:\r\n",
    "            PO_best_score = PO_val_score\r\n",
    "            POmodel.save_weights('opt/ml/processing/model/polarity_model.h5')\r\n",
    "            print(f\"INFO -- FOLD {fold+1} polarity model weights saved.\")\r\n",
    "        else:\r\n",
    "            print(\"INFO -- Model weights were not replaced.\")\r\n",
    "\r\n",
    "    save_tarfile(\r\n",
    "        outfile=\"opt/ml/processing/output/sentiment_models.tar.gz\",\r\n",
    "        model_dir=\"opt/ml/processing/model\")\r\n",
    "    print(\"INFO -- Successfully compressed models into tar.\")\r\n",
    "    print(\"Training completed.\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO -- Successfully fetched data from S3 bucket.\n",
      "INFO -- Successfully compressed models into tar.\n",
      "Training completed.\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('tf-gpu': conda)"
  },
  "interpreter": {
   "hash": "b851d2923cfa3a2562599062e05fd9893d86a7c009c64d8ad3756552e4dd5f41"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}