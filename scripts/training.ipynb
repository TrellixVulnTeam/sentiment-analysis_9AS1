{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "import tarfile\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import (\n",
    "    TFAutoModelForQuestionAnswering, \n",
    "    TFAutoModelForSequenceClassification,\n",
    "    AutoConfig)\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "\n",
    "def seed_everything(SEED):\n",
    "   random.seed(SEED)\n",
    "   np.random.seed(SEED)\n",
    "   tf.random.set_seed(SEED)\n",
    "   print(\"INFO -- Random seed set.\")\n",
    "\n",
    "\n",
    "def get_data(input_path, max_len):\n",
    "    train_data = pd.read_csv(input_path)\n",
    "    for col in train_data.columns[3:]:\n",
    "        train_data[col] = train_data[col].apply(ast.literal_eval).apply(np.array)\n",
    "        if col in train_data.columns[3:-2]:\n",
    "            train_data[col] = train_data[col].apply(lambda x: x[:max_len])\n",
    "    return train_data\n",
    "\n",
    "\n",
    "def get_input_arrays(df, type, batch_size, buffer_size, training_set):\n",
    "    ids = np.vstack(df['input_ids'])\n",
    "    mask = np.vstack(df['attention_mask'])\n",
    "    start = np.vstack(df['start_token'])\n",
    "    end = np.vstack(df['end_token'])\n",
    "    sentiment = np.vstack(df['sentiment_token'])\n",
    "    ids_PO = np.vstack(df['input_ids2'])\n",
    "    mask_PO = np.vstack(df['attention_mask2'])\n",
    "    sentiment_PO = df['sentiment'].astype('int8')\n",
    "\n",
    "    if type==\"QA\":\n",
    "        gen = tf.data.Dataset.from_tensor_slices((\n",
    "            {\n",
    "                \"input_1\": ids,\n",
    "                \"input_2\": mask,\n",
    "                \"input_3\": sentiment\n",
    "            },\n",
    "            {\n",
    "                \"output_1\": tf.convert_to_tensor(start, dtype=tf.int32),\n",
    "                \"output_2\": tf.convert_to_tensor(end, dtype=tf.int32)\n",
    "            }\n",
    "        ))\n",
    "\n",
    "    else:\n",
    "        gen = tf.data.Dataset.from_tensor_slices((\n",
    "            {\n",
    "                \"input_1\": ids_PO,\n",
    "                \"input_2\": mask_PO\n",
    "            }, tf.one_hot(sentiment_PO, depth=3)\n",
    "        ))\n",
    "\n",
    "    gen = gen.shuffle(buffer_size=256)\n",
    "    gen = gen.batch(batch_size=batch_size)\n",
    "    gen = gen.prefetch(buffer_size=buffer_size)\n",
    "    return gen\n",
    "\n",
    "\n",
    "def QAModel(pretrained, max_len):\n",
    "    input_ids = tf.keras.layers.Input(shape=(max_len,), name=\"input_1\",  dtype=tf.int32)\n",
    "    att_mask = tf.keras.layers.Input(shape=(max_len,), name=\"input_2\", dtype=tf.int32)\n",
    "    sent_mask = tf.keras.layers.Input(shape=(max_len,), name=\"input_3\", dtype=tf.int32)\n",
    "    \n",
    "    config = AutoConfig.from_pretrained(\n",
    "        pretrained, \n",
    "        output_attention=True, \n",
    "        output_hidden_states=True, \n",
    "        use_cache=True)\n",
    "\n",
    "    enc = TFAutoModelForQuestionAnswering.from_pretrained(\n",
    "        pretrained, config=config)\n",
    "    x = enc(input_ids, attention_mask=att_mask, token_type_ids=sent_mask)\n",
    "\n",
    "    x1 = tf.keras.layers.Dropout(0.1)(x[0])\n",
    "    x1 = tf.expand_dims(x1, axis=-1)\n",
    "    x1 = tf.keras.layers.Conv1D(1,1)(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x[0])\n",
    "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
    "\n",
    "    x2 = tf.keras.layers.Dropout(0.1)(x[1])\n",
    "    x2 = tf.expand_dims(x2, axis=-1)\n",
    "    x2 = tf.keras.layers.Conv1D(1,1)(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[input_ids, att_mask, sent_mask], outputs=[x1,x2])\n",
    "\n",
    "    for layer in model.layers[3:4]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def POModel(max_len, pretrained):\n",
    "    input_ids = tf.keras.layers.Input(shape=(max_len,), name='input_1', dtype=tf.int32)\n",
    "    att_mask = tf.keras.layers.Input(shape=(max_len,), name='input_2', dtype=tf.int32)\n",
    "\n",
    "    enc = TFAutoModelForSequenceClassification.from_pretrained(pretrained, num_labels=3)\n",
    "    x = enc(input_ids, attention_mask=att_mask)[0]\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dense(256, activation=None)(x)\n",
    "    x = tf.keras.layers.Dropout(0.4)(x)\n",
    "    x = tf.keras.layers.Dense(3, activation='softmax')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[input_ids, att_mask], outputs=x)\n",
    "\n",
    "    for layer in model.layers[:3]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def QAtraining(epochs, max_len, learning_rate, train_gen, val_gen):\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
    "            initial_learning_rate=learning_rate, \n",
    "            first_decay_steps=3,\n",
    "            t_mul=2.0, m_mul=1.0, alpha=0.0))\n",
    "\n",
    "    cce_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=False, label_smoothing=0.2)\n",
    "    model = QAModel(pretrained='bert-base-uncased', max_len=max_len)\n",
    "    t_jaccard_metric = tf.keras.metrics.MeanIoU(num_classes=2)\n",
    "    v_jaccard_metric = tf.keras.metrics.MeanIoU(num_classes=2)\n",
    "    metrics = ['loss', 'jaccard', 'val_loss', 'val_jaccard']\n",
    "    best_score = 0\n",
    "\n",
    "    t_jaccard_metric.reset_states()\n",
    "    v_jaccard_metric.reset_states()\n",
    "\n",
    "    @tf.function\n",
    "    def getArrays(array):\n",
    "        start_array = tf.where(\n",
    "            tf.sequence_mask(\n",
    "                lengths=tf.math.argmax(array[0], axis=1), \n",
    "                maxlen=max_len), \n",
    "            x=1, y=0)\n",
    "        end_array = tf.where(\n",
    "            tf.sequence_mask(\n",
    "                lengths=tf.math.argmax(array[1], axis=1),\n",
    "                maxlen=max_len),\n",
    "            x=1, y=0)\n",
    "        return tf.abs(end_array - start_array)\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(X, y, loss_fn):\n",
    "        y = [y['output_1'], y['output_2']]\n",
    "        with tf.GradientTape() as tape:        \n",
    "            y_hat = model(X, training=True)\n",
    "            loss_value = loss_fn(y, y_hat)\n",
    "            loss_value += sum(model.losses)\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        t_jaccard_metric.update_state(getArrays(y), getArrays(y_hat))\n",
    "        return loss_value\n",
    "\n",
    "    @tf.function\n",
    "    def val_step(X, y, loss_fn):\n",
    "        y = [y['output_1'], y['output_2']]\n",
    "        y_hat = model(X, training=False)\n",
    "        loss_value = loss_fn(y, y_hat)\n",
    "        v_jaccard_metric.update_state(getArrays(y), getArrays(y_hat))\n",
    "        return loss_value\n",
    "\n",
    "    i = 0\n",
    "    for epoch in range(1, epochs+1):\n",
    "        print(f\"Epoch {epoch}/{epochs}\")\n",
    "        progbar = tf.keras.utils.Progbar(\n",
    "            len(train_gen), interval=0.5, stateful_metrics=metrics)\n",
    "        print(f\"INFO -- Learning rate set to {optimizer.lr(epoch)}.\")\n",
    "        for step, (X_train, y_train) in enumerate(train_gen):\n",
    "            train_loss = train_step(X_train, y_train, cce_fn)\n",
    "            progbar.update(step, values=[('loss', train_loss), ('jaccard', t_jaccard_metric.result())])\n",
    "\n",
    "        for x_batch_val, y_batch_val in val_gen:\n",
    "            val_loss = val_step(x_batch_val, y_batch_val, cce_fn)\n",
    "\n",
    "        values = [\n",
    "            ('loss', train_loss), \n",
    "            ('jaccard', t_jaccard_metric.result()),\n",
    "            ('val_loss', val_loss),\n",
    "            ('val_jaccard', v_jaccard_metric.result())]\n",
    "        progbar.update(len(train_gen), values=values, finalize=True)\n",
    "\n",
    "        if best_score < v_jaccard_metric.result():\n",
    "            best_score = v_jaccard_metric.result()\n",
    "            best_model = model\n",
    "            print(\"INFO -- Model improved.\\n\")\n",
    "            i = 0\n",
    "        else:\n",
    "            i += 1\n",
    "            if (i < 2) & (v_jaccard_metric.result() < t_jaccard_metric.result()):\n",
    "                print(\"INFO -- Model did not improve.\\n\")\n",
    "            else:\n",
    "                print(\"INFO -- Early Stopping.\\n\")\n",
    "                break\n",
    "\n",
    "    return best_model, best_score\n",
    "\n",
    "\n",
    "def POtraining(epochs, max_len, learning_rate, train_gen, val_gen):\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=learning_rate)\n",
    "\n",
    "    cce_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=False, label_smoothing=0.2)\n",
    "    model = POModel(pretrained='distilroberta-base', max_len=max_len)\n",
    "    t_f1_metric = tfa.metrics.F1Score(num_classes=3, average='micro')\n",
    "    v_f1_metric = tfa.metrics.F1Score(num_classes=3, average='micro')\n",
    "    metrics = ['loss', 'f1', 'val_loss', 'val_f1']\n",
    "    best_score = 0\n",
    "\n",
    "    t_f1_metric.reset_states()\n",
    "    v_f1_metric.reset_states()\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(X, y, loss_fn):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_hat = model(X, training=True)\n",
    "            loss_value = loss_fn(y, y_hat)\n",
    "            loss_value += sum(model.losses)\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        t_f1_metric.update_state(y, y_hat)\n",
    "        return loss_value\n",
    "\n",
    "    @tf.function\n",
    "    def val_step(X, y, loss_fn):\n",
    "        y_hat = model(X, training=False)\n",
    "        loss_value = loss_fn(y, y_hat)\n",
    "        v_f1_metric.update_state(y, y_hat)\n",
    "        return loss_value\n",
    "\n",
    "    i = 0\n",
    "    for epoch in range(1, epochs+1):\n",
    "        print(f\"Epoch {epoch}/{epochs}\")\n",
    "        progbar = tf.keras.utils.Progbar(\n",
    "            len(train_gen), interval=0.5, stateful_metrics=metrics)\n",
    "        for step, (X_train, y_train) in enumerate(train_gen):\n",
    "            train_loss = train_step(X_train, y_train, cce_fn)\n",
    "            progbar.update(step, values=[('loss', train_loss), ('f1', t_f1_metric.result())])\n",
    "\n",
    "        for x_batch_val, y_batch_val in val_gen:\n",
    "            val_loss = val_step(x_batch_val, y_batch_val, cce_fn)\n",
    "\n",
    "        values = [\n",
    "            ('loss', train_loss),\n",
    "            ('f1', t_f1_metric.result()),\n",
    "            ('val_loss', val_loss),\n",
    "            ('val_f1', v_f1_metric.result())]\n",
    "        progbar.update(len(train_gen), values=values, finalize=True)\n",
    "\n",
    "        if ((t_f1_metric.result() - v_f1_metric.result()) < 0.02) & (v_f1_metric.result() > best_score):\n",
    "            best_score = v_f1_metric.result()\n",
    "            best_model = model\n",
    "            print(\"INFO -- Model improved.\\n\")\n",
    "            i = 0\n",
    "        else:\n",
    "            i += 1\n",
    "            if i < 2:\n",
    "                print(\"INFO -- Model did not improve.\\n\")\n",
    "            else:\n",
    "                print(\"INFO -- Early Stopping.\\n\")\n",
    "                break\n",
    "\n",
    "    return best_model, best_score\n",
    "    \n",
    "\n",
    "def save_tarfile(outfile, model_dir):\n",
    "    with tarfile.open(outfile, \"w:gz\") as tar:\n",
    "        for file in os.listdir(model_dir):\n",
    "            tar.add(\n",
    "                name=os.path.join(model_dir, file), \n",
    "                arcname=file,\n",
    "                recursive=False)\n",
    "        tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Dependencies ------\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2020 NVIDIA Corporation\n",
      "Built on Mon_Oct_12_20:54:10_Pacific_Daylight_Time_2020\n",
      "Cuda compilation tools, release 11.1, V11.1.105\n",
      "Build cuda_11.1.relgpu_drvr455TC455_06.29190527_0\n",
      "\n",
      "Python version: 3.8.10 (default, May 19 2021, 13:12:57) [MSC v.1916 64 bit (AMD64)]\n",
      "Tensorflow version: 2.5.0\n",
      "INFO -- Mixed precision set.\n",
      "INFO -- Random seed set.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import argparse\n",
    "import subprocess\n",
    "import tensorflow as tf\n",
    "\n",
    "# Environ set-up\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "print(\"------ Dependencies ------\")\n",
    "print(subprocess.check_output('nvcc --version'.split(' ')).decode())\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"Tensorflow version:\", tf.__version__)\n",
    "\n",
    "# Mixed precision\n",
    "try:\n",
    "    policy = tf.keras.mixed_precision.Policy('float32')\n",
    "    tf.keras.mixed_precision.set_global_policy(policy)\n",
    "    print(\"INFO -- Mixed precision set.\")\n",
    "except:\n",
    "    print(\"ERROR -- Mixed precision not set.\")\n",
    "\n",
    "SEED = 42\n",
    "seed_everything(SEED)\n",
    "buffer_size = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO -- Successfully fetched data from S3 bucket.\n",
      "INFO -- Successfully compressed models into tar.\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--access_id\", dest=\"access_id\", type=str)\n",
    "    parser.add_argument(\n",
    "        \"--access_key\", dest=\"access_key\", type=str)\n",
    "    parser.add_argument(\n",
    "        \"--max_length\", dest=\"max_len\", type=int, default=64)    \n",
    "    parser.add_argument(\n",
    "        \"--batch_size\", dest=\"batch_size\", type=int, default=32)\n",
    "    parser.add_argument(\n",
    "        \"--epochs\", dest=\"epochs\", type=int, default=20)\n",
    "    parser.add_argument(\n",
    "        \"--lr\", dest='lr', type=float, default=1e-5)\n",
    "    parser.add_argument(\n",
    "        \"--splits\", dest='splits', type=int, default=5)\n",
    "    parser.add_argument(\n",
    "        \"--pretrained_QA\", dest='pretrained_QA', type=str, \n",
    "        default='bert-base-uncased')\n",
    "    parser.add_argument(\n",
    "        \"--pretrained_PO\", dest='pretrained_PO', type=str, \n",
    "        default='distilroberta-base')   \n",
    "    parser.add_argument(\n",
    "        \"--bucket_name\", dest=\"bucket_name\", type=str, \n",
    "        default=\"syalabi-bucket\")\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    try: \n",
    "        train_data = get_data(\n",
    "            input_path='input/train_cleaned.csv',\n",
    "            max_len=args.max_len)\n",
    "        print(\"INFO -- Successfully fetched data from S3 bucket.\")\n",
    "    except:\n",
    "        print('ERROR -- Fetching data from S3 bucket has failed.')\n",
    "\n",
    "    QA_best_score, PO_best_score = 0, 0\n",
    "    skf = StratifiedKFold(n_splits=args.splits, shuffle=True, random_state=SEED)\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(\n",
    "            X=train_data.drop('sentiment', axis=1),\n",
    "            y=train_data['sentiment'])):\n",
    "        \n",
    "        QAtrain = get_input_arrays(\n",
    "            train_data.loc[train_idx, :].reset_index(drop=True), \n",
    "            type=\"QA\",\n",
    "            batch_size=args.batch_size,\n",
    "            buffer_size=buffer_size,\n",
    "            training_set=True)\n",
    "\n",
    "        QAval = get_input_arrays(\n",
    "            train_data.loc[val_idx, :].reset_index(drop=True),\n",
    "            type=\"QA\",\n",
    "            batch_size=args.batch_size,\n",
    "            buffer_size=buffer_size,\n",
    "            training_set=False)\n",
    "\n",
    "        POtrain = get_input_arrays(\n",
    "            train_data.loc[train_idx, :].reset_index(drop=True), \n",
    "            type=\"PO\",\n",
    "            batch_size=args.batch_size,\n",
    "            buffer_size=buffer_size,\n",
    "            training_set=True)\n",
    "\n",
    "        POval = get_input_arrays(\n",
    "            train_data.loc[val_idx, :].reset_index(drop=True), \n",
    "            type=\"PO\",\n",
    "            batch_size=args.batch_size,\n",
    "            buffer_size=buffer_size,\n",
    "            training_set=False)\n",
    "\n",
    "        # Phrase Loop\n",
    "        print(f'INFO -- Phrase Training Fold {fold+1} of {args.splits}.')\n",
    "        QAmodel, QA_val_score = QAtraining(\n",
    "            epochs=args.epochs, \n",
    "            max_len=args.max_len,\n",
    "            learning_rate=args.lr,\n",
    "            train_gen=QAtrain,\n",
    "            val_gen=QAval)\n",
    "       \n",
    "        if QA_val_score > QA_best_score:\n",
    "            QA_best_score = QA_val_score\n",
    "            QAmodel.save_weights('opt/ml/processing/model/phrase_model.h5')\n",
    "            print(f\"INFO -- FOLD {fold+1} phrase model weights saved.\")\n",
    "        else:\n",
    "            print(\"INFO -- Model weights were not replaced.\")\n",
    "\n",
    "        # Polarity Loop\n",
    "        tf.keras.backend.clear_session()\n",
    "        print(f'INFO -- Polarity Training Fold {fold+1} of {args.splits}.')\n",
    "        \n",
    "        POmodel, PO_val_score = POtraining(\n",
    "            epochs=args.epochs, \n",
    "            max_len=args.max_len,\n",
    "            learning_rate=args.lr,\n",
    "            train_gen=POtrain,\n",
    "            val_gen=POval)\n",
    "\n",
    "        if PO_val_score > PO_best_score:\n",
    "            PO_best_score = PO_val_score\n",
    "            POmodel.save_weights('opt/ml/processing/model/polarity_model.h5')\n",
    "            print(f\"INFO -- FOLD {fold+1} polarity model weights saved.\")\n",
    "        else:\n",
    "            print(\"INFO -- Model weights were not replaced.\")\n",
    "\n",
    "    save_tarfile(\n",
    "        outfile=\"opt/ml/processing/output/sentiment_models.tar.gz\",\n",
    "        model_dir=\"opt/ml/processing/model\")\n",
    "    print(\"INFO -- Successfully compressed models into tar.\")\n",
    "    print(\"Training completed.\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b851d2923cfa3a2562599062e05fd9893d86a7c009c64d8ad3756552e4dd5f41"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('tf-gpu': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
